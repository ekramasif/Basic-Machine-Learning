{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_Text_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekramasif/Basic-Machine-Learning/blob/main/Simple_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUee8aFXpH17"
      },
      "source": [
        "import re"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76eUmE5pOvq3",
        "outputId": "6d3cc8d4-32d9-48fa-c432-ea7178043ffe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/AI_LAB_Fall_2020/"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/AI_LAB_Fall_2020/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4QCWdk8N-ra"
      },
      "source": [
        "def read_files(file_loc):\n",
        "  dataset = []\n",
        "  \n",
        "  with open(file_loc, 'r', encoding='utf-8') as test_file:\n",
        "    for line in test_file:\n",
        "      dataset.append(line)\n",
        "    \n",
        "  return dataset"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVqh8cxQTJOa"
      },
      "source": [
        "def separate_labels(dataset):\n",
        "  documents = []\n",
        "  labels = []\n",
        "\n",
        "  for line in dataset:\n",
        "    splitted_line = line.strip().split('\\t', 2)\n",
        "    labels.append(splitted_line[1])\n",
        "    documents.append(splitted_line[2])\n",
        "\n",
        "  return documents, labels"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b33fQkNF-nBX",
        "outputId": "0548a98f-8dd5-42de-a7eb-745103b26b87"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLOZfny8UbwB"
      },
      "source": [
        "def remove_urls(data):\n",
        "  url_removed_data = []\n",
        "\n",
        "  # Your Code Here...\n",
        "  for line in data:\n",
        "    url_removed_data.append(re.sub('http[s]?://\\S+', '', line))\n",
        "\n",
        "  return url_removed_data"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1LEQlJAUfeH"
      },
      "source": [
        "def remove_hashtag(data):\n",
        "  '''This function removes HashTag # from the entire dataset'''\n",
        "  hashtag_removed_data = []\n",
        "\n",
        "  # map hashtag to space\n",
        "  translator = str.maketrans('#', ' '*len('#'), '')\n",
        "\n",
        "  for line in data:\n",
        "    hashtag_removed_data.append(line.translate(translator))\n",
        "\n",
        "  return hashtag_removed_data"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPy53mwTIt9Y"
      },
      "source": [
        "# remove whitespace from text \n",
        "def remove_whitespace(data):\n",
        "  '''This function removes multiple whitespaces and replace them with a single whitespace'''\n",
        "  whitespace_removed_data = []\n",
        "  \n",
        "  for line in data:\n",
        "    whitespace_removed_data.append(\" \".join(line.split()))\n",
        "  \n",
        "  return whitespace_removed_data"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3UHR8hNUSSa"
      },
      "source": [
        "def pre_processing(documents):\n",
        "  '''This function calls all the pre-processing submodules'''\n",
        "\n",
        "  documents = remove_urls(documents)\n",
        "  documents = remove_hashtag(documents)\n",
        "  documents = remove_whitespace(documents)\n",
        "\n",
        "  return documents"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f-LKvxSN0T2",
        "outputId": "712b193c-770e-4131-d2aa-7ed4955ea3d7"
      },
      "source": [
        "def main():\n",
        "  print(\"Reading The Dataset ...\")\n",
        "  dataset = read_files('/content/drive/MyDrive/Colab Notebooks/datasets/corona_data/train.tsv')\n",
        "  documents, labels = separate_labels(dataset)\n",
        "\n",
        "  # print(dataset)\n",
        "  \n",
        "  print('\\nBefore Preprocessing:')\n",
        "  print(documents[0])\n",
        "\n",
        "  documents = pre_processing(documents)\n",
        "\n",
        "  print('\\nAfter Preprocessing:')\n",
        "  print(documents[0])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading The Dataset ...\n",
            "\n",
            "Before Preprocessing:\n",
            "@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n",
            "\n",
            "After Preprocessing:\n",
            "@MeNyrbie @Phil_Gahan @Chrisitv and and\n"
          ]
        }
      ]
    }
  ]
}